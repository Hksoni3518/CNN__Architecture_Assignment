{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment"
      ],
      "metadata": {
        "id": "tdHAMprDBf9S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.1. What is the role of filters and feature maps in Convolutional Neural\n",
        "Network (CNN)?\n",
        "\n",
        "Answer ->>\n",
        "\n",
        "In a Convolutional Neural Network (CNN), filters and feature maps play the most important roles in extracting meaningful patterns from images (or other grid-like data).\n",
        "\n",
        "1. Filters (also called kernels) :-  \n",
        "\n",
        "- A filter is a small matrix of learnable weights (e.g., 3×3 or 5×5) that slides over the input image.\n",
        "\n",
        "- Role of Filters :\n",
        "\n",
        "    - Extract features such as edges, textures, curves, and patterns.\n",
        "\n",
        "    - Each filter focuses on a different type of feature.\n",
        "\n",
        "    - The values inside the filter get updated during training to detect useful patterns for the task (e.g., classification).\n",
        "\n",
        "- Example:\n",
        "\n",
        "    - One filter may detect horizontal edges.\n",
        "\n",
        "    - Another may detect vertical edges.\n",
        "\n",
        "    - Another may detect corners or color gradients.\n",
        "\n",
        "2. Feature Maps (also called activation maps or output maps) :-   \n",
        "\n",
        "- A feature map is the output produced after a filter slides over the input and performs convolution operations.\n",
        "\n",
        "- Role of Feature Maps :    \n",
        "\n",
        "    - They show where in the input a particular feature was detected by a filter.\n",
        "\n",
        "    - They represent presence and strength of detected features.\n",
        "\n",
        "    - Feature maps get passed to deeper layers, which detect more complex and abstract patterns.\n",
        "\n",
        "- Example:\n",
        "\n",
        "    - Shallow layer feature maps → detect edges, simple shapes.\n",
        "\n",
        "    - Deeper layer feature maps → detect eyes, nose, textures.\n",
        "\n",
        "    - Final layers → detect high-level objects like \"cat\", \"car\", \"face\"."
      ],
      "metadata": {
        "id": "F8sVIuS3Bf_0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.2. Explain the concepts of padding and stride in CNNs(Convolutional Neural\n",
        "Network). How do they affect the output dimensions of feature maps?\n",
        "\n",
        "Answer ->>\n",
        "\n",
        "**Padding in CNN :**\n",
        "\n",
        "- Padding means adding extra pixels (usually zeros) around the border of the input image.\n",
        "\n",
        "- Types of Padding :\n",
        "\n",
        "    1. Valid Padding (No Padding) :    \n",
        "\n",
        "    - No extra pixels are added\n",
        "\n",
        "    - Output shrinks\n",
        "\n",
        "    - Called “valid” because only valid pixels are used\n",
        "\n",
        "    2. Same Padding (Zero Padding)\n",
        "\n",
        "    - Padding is added so output has same size as input\n",
        "\n",
        "    - Used commonly in deep CNNs (e.g., in TensorFlow, SAME padding)\n",
        "\n",
        "- Effect on Output Size :\n",
        "\n",
        "- If:\n",
        "\n",
        "    - Input size = N\n",
        "\n",
        "    - Filter size = F\n",
        "\n",
        "    - Padding = P\n",
        "\n",
        "    - Stride = S\n",
        "\n",
        "    - Then output size:\n",
        "\n",
        "          Output size= ⌊(N−F+2P)/2​⌋+1\n",
        "\n",
        "**Stride in CNN :**\n",
        "\n",
        "- Stride refers to how many pixels the filter moves each time during convolution.\n",
        "\n",
        "- Effects of Stride :      \n",
        "\n",
        "    - Stride = 1 → Filter moves one pixel at a time\n",
        "\n",
        "      → High resolution, bigger output\n",
        "\n",
        "    - Stride = 2 or more → Filter jumps over pixels\n",
        "\n",
        "      → Shrinks the output\n",
        "\n",
        "      → Acts similar to downsampling\n",
        "\n",
        "- Example:\n",
        "\n",
        "    - If stride = 1\n",
        "\n",
        "      → The filter overlaps a lot, output is large\n",
        "\n",
        "    - If stride = 2\n",
        "      → Filter jumps by 2 pixels, output becomes smaller"
      ],
      "metadata": {
        "id": "iM9JJItZBgC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.3.  Define receptive field in the context of CNNs. Why is it important for deep\n",
        "architectures?\n",
        "\n",
        "Answer ->>\n",
        "\n",
        "**Receptive Field in CNNs :**\n",
        "\n",
        "The receptive field of a neuron in a CNN is the region of the input image that affects (or contributes to) the value of that neuron.\n",
        "\n",
        "In simple words:\n",
        "\n",
        "- It is the area of the input that a particular feature in a deeper layer \"sees.\"\n",
        "\n",
        "- Every neuron does not see the entire image directly; it sees only a local patch.\n",
        "\n",
        "- As we go deeper, each neuron indirectly sees larger and larger areas of the original image.\n",
        "\n",
        "Example\n",
        "\n",
        "- In the first convolution layer, a neuron may see a 3×3 region of the input.\n",
        "\n",
        "- In layer 3, due to stacking, the neuron may effectively see a 9×9 or 15×15 region of the input.\n",
        "\n",
        "- In very deep layers, the receptive field may cover the whole image.\n",
        "\n",
        "**Why is Receptive Field Important in Deep Architectures :**\n",
        "\n",
        "1. Captures larger and more complex patterns\n",
        "2. Helps the network understand spatial relationships\n",
        "3. Required for semantic understanding\n",
        "4. Prevents missing global features\n",
        "5. Deeper layers grow receptive fields naturally"
      ],
      "metadata": {
        "id": "wkaH6CPnBgFI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.4. Discuss how filter size and stride influence the number of parameters in a\n",
        "CNN.\n",
        "\n",
        "Answer ->>\n",
        "\n",
        "**Filter Size and Number of Parameters :**\n",
        "\n",
        "**What determines the number of parameters?**\n",
        "\n",
        "  In a convolution layer, the number of parameters depends on:\n",
        "\n",
        "    Parameters=(F×F×Cin​+1)×Cout​\n",
        "\n",
        "  Here :    \n",
        "\n",
        "    F = filter size (e.g., 3×3, 5×5)\n",
        "\n",
        "    Cin = number of input channels\n",
        "\n",
        "    Cout = number of filters (output channels)\n",
        "\n",
        "    +1 is for bias term\n",
        "\n",
        "**How Filter Size Affects Parameters?**\n",
        "\n",
        "Larger filter size = more parameters\n",
        "\n",
        "Example (with 3 input channels):\n",
        "\n",
        "  -  With 3*3 filter:\n",
        "\n",
        "      Parameters per filter = 3 * 3 * 3 = 27\n",
        "\n",
        "  - With 5*5 filter:\n",
        "\n",
        "      Parameters per filter = 5 * 5 * 3 = 75\n",
        "\n",
        "- Increasing the filter size increases the parameters quadratically.\n",
        "- If you have many filters, total parameters increase dramatically."
      ],
      "metadata": {
        "id": "E9IxQCEnBgHW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.5. Compare and contrast different CNN-based architectures like LeNet,\n",
        "AlexNet, and VGG in terms of depth, filter sizes, and performance.\n",
        "\n",
        "Answer ->>\n",
        "\n",
        "**LeNet-5 (1998) :**\n",
        "\n",
        "Architectural Features :     \n",
        "\n",
        "- One of the earliest CNNs (Yann LeCun)\n",
        "\n",
        "- Designed for MNIST digit recognition\n",
        "\n",
        "- Very shallow network\n",
        "\n",
        "Depth :      \n",
        "\n",
        "- 7 layers (including conv + pooling + fully connected)\n",
        "\n",
        "Filter Sizes :     \n",
        "\n",
        "- Convolution filters: 5×5\n",
        "\n",
        "- Stride: small (usually 1)\n",
        "\n",
        "- Pooling: average pooling (subsampling)\n",
        "\n",
        "Parameters :    \n",
        "\n",
        "- ~ 60,000 parameters\n",
        "\n",
        "Performance :   \n",
        "\n",
        "- Excellent for small grayscale images (e.g., MNIST)\n",
        "\n",
        "- Not suitable for large or complex datasets.\n",
        "\n",
        "**AlexNet (2012) :**\n",
        "\n",
        "Architectural Features :   \n",
        "\n",
        "- Winner of ImageNet 2012 (Krizhevsky, Sutskever, Hinton)\n",
        "\n",
        "- A breakthrough that revived deep learning\n",
        "\n",
        "- Introduced ReLU activation, dropout, and GPU training\n",
        "\n",
        "Depth :      \n",
        "\n",
        "- 8 learnable layers\n",
        "\n",
        "- 5 convolutional layers\n",
        "\n",
        "- 3 fully connected layers\n",
        "\n",
        "Filter Sizes :      \n",
        "\n",
        "- First conv layer: 11×11 filters, stride 4\n",
        "\n",
        "- Later layers: 5×5 and 3×3 filters\n",
        "\n",
        "- Max pooling used instead of average pooling\n",
        "\n",
        "Parameters :       \n",
        "\n",
        "- ~ 60 million parameters\n",
        "\n",
        "Performance :     \n",
        "\n",
        "- Achieved top-5 error of ~15% (huge improvement over competitors)\n",
        "\n",
        "- Works well for large-scale image classification\n",
        "\n",
        "**VGG (VGG-16 / VGG-19, 2014) :**\n",
        "\n",
        "Architectural Features :      \n",
        "\n",
        "- Developed by Oxford’s Visual Geometry Group (VGG)\n",
        "\n",
        "- Uses very deep networks with simple, uniform architecture\n",
        "\n",
        "- Replaces large filters with many 3×3 filters\n",
        "\n",
        "Depth :     \n",
        "\n",
        "- 16 or 19 layers (very deep)\n",
        "\n",
        "- VGG16 → 13 conv + 3 fully connected\n",
        "\n",
        "- VGG19 → 16 conv + 3 fully connected\n",
        "\n",
        "Filter Sizes :      \n",
        "\n",
        "- All convolution filters are 3×3\n",
        "\n",
        "- Stride = 1\n",
        "\n",
        "- Consistent use of max pooling\n",
        "\n",
        "Parameters :      \n",
        "\n",
        "- Very large:\n",
        "\n",
        "    - VGG16 → ~ 138 million parameters\n",
        "\n",
        "    - VGG19 → ~ 144 million parameters\n",
        "\n",
        "Performance :      \n",
        "\n",
        "- Very high accuracy on ImageNet\n",
        "\n",
        "- Performs significantly better than AlexNet\n",
        "\n",
        "- But computationally heavy & memory intensive"
      ],
      "metadata": {
        "id": "71Jei5HpBgJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.6. Using keras, build and train a simple CNN model on the MNIST dataset\n",
        "from scratch. Include code for module creation, compilation, training, and evaluation.\n",
        "\n",
        "Answer->>"
      ],
      "metadata": {
        "id": "6bXcULQwBgMH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# 1. Import Required Libraries\n",
        "# ---------------------------\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "# ---------------------------\n",
        "# 2. Load and Preprocess Data\n",
        "# ---------------------------\n",
        "# Load dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Reshape to (samples, height, width, channels)\n",
        "x_train = x_train.reshape((x_train.shape[0], 28, 28, 1))\n",
        "x_test  = x_test.reshape((x_test.shape[0], 28, 28, 1))\n",
        "\n",
        "# Normalize pixel values\n",
        "x_train = x_train.astype(\"float32\") / 255\n",
        "x_test  = x_test.astype(\"float32\") / 255\n",
        "\n",
        "# One-hot encode labels\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test  = to_categorical(y_test, 10)\n",
        "\n",
        "# ---------------------------\n",
        "# 3. Build CNN Model\n",
        "# ---------------------------\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),\n",
        "    MaxPooling2D(pool_size=(2,2)),\n",
        "\n",
        "    Conv2D(64, (3,3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2,2)),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "\n",
        "    Dense(10, activation='softmax')  # Output layer\n",
        "])\n",
        "\n",
        "# ---------------------------\n",
        "# 4. Compile Model\n",
        "# ---------------------------\n",
        "model.compile(optimizer=\"adam\",\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "# Display model summary\n",
        "model.summary()\n",
        "\n",
        "# ---------------------------\n",
        "# 5. Train the Model\n",
        "# ---------------------------\n",
        "history = model.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=10,\n",
        "    batch_size=128,\n",
        "    validation_split=0.1,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "# ---------------------------\n",
        "# 6. Evaluate on Test Data\n",
        "# ---------------------------\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Test Accuracy:\", test_acc)\n",
        "print(\"Test Loss:\", test_loss)\n"
      ],
      "metadata": {
        "id": "Oyyg95Fg-nib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.7. Load and preprocess the CIFAR-10 dataset using Keras, and create a\n",
        "CNN model to classify RGB images. Show your preprocessing and architecture.\n",
        "\n",
        "Answer ->>"
      ],
      "metadata": {
        "id": "VD-TUBAIBgPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Import Libraries\n",
        "# ---------------------------\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n"
      ],
      "metadata": {
        "id": "QScIi_4eZGfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------\n",
        "# Load CIFAR-10 dataset\n",
        "# ---------------------------------\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "print(\"Training data shape:\", x_train.shape)   # (50000, 32, 32, 3)\n",
        "print(\"Test data shape:\", x_test.shape)       # (10000, 32, 32, 3)\n"
      ],
      "metadata": {
        "id": "bgiTDM-bZa4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "x_test  = x_test.astype(\"float32\") / 255.0\n"
      ],
      "metadata": {
        "id": "QQ2UzrL3Zdm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = to_categorical(y_train, 10)\n",
        "y_test  = to_categorical(y_test, 10)\n"
      ],
      "metadata": {
        "id": "QZcMoTFIZgR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------\n",
        "# Build CNN model\n",
        "# ---------------------------------\n",
        "model = Sequential([\n",
        "\n",
        "    # Block 1\n",
        "    Conv2D(32, (3,3), padding='same', activation='relu', input_shape=(32,32,3)),\n",
        "    BatchNormalization(),\n",
        "    Conv2D(32, (3,3), padding='same', activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D((2,2)),\n",
        "    Dropout(0.25),\n",
        "\n",
        "    # Block 2\n",
        "    Conv2D(64, (3,3), padding='same', activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Conv2D(64, (3,3), padding='same', activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D((2,2)),\n",
        "    Dropout(0.25),\n",
        "\n",
        "    # Block 3\n",
        "    Conv2D(128, (3,3), padding='same', activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Conv2D(128, (3,3), padding='same', activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D((2,2)),\n",
        "    Dropout(0.25),\n",
        "\n",
        "    # Classifier\n",
        "    Flatten(),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n"
      ],
      "metadata": {
        "id": "d4BfZe_VZiUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "ExbiuZFmZlVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=20,\n",
        "    batch_size=64,\n",
        "    validation_split=0.1,\n",
        "    verbose=2\n",
        ")\n"
      ],
      "metadata": {
        "id": "mCo3Cnvzal1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Test Accuracy:\", test_acc)\n",
        "print(\"Test Loss:\", test_loss)\n"
      ],
      "metadata": {
        "id": "Qdz7ZIMtZqlx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.8. Using PyTorch, write a script to define and train a CNN on the MNIST\n",
        "dataset. Include model definition, data loaders, training loop, and accuracy evaluation.\n",
        "\n",
        "Answer ->>"
      ],
      "metadata": {
        "id": "pm5OsYHsZ3tN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 1. Device configuration\n",
        "# ----------------------------------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 2. Transform & Load MNIST Dataset\n",
        "# ----------------------------------------------------\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),                       # convert to tensor\n",
        "    transforms.Normalize((0.1307,), (0.3081,))   # normalize MNIST data\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(\n",
        "    root='./data', train=True, download=True, transform=transform\n",
        ")\n",
        "\n",
        "test_dataset = datasets.MNIST(\n",
        "    root='./data', train=False, download=True, transform=transform\n",
        ")\n",
        "\n",
        "# DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 3. Define CNN Model\n",
        "# ----------------------------------------------------\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        self.conv_layer = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, padding=1),  # (32, 28, 28)\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),                             # (32, 14, 14)\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1), # (64, 14, 14)\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)                              # (64, 7, 7)\n",
        "        )\n",
        "\n",
        "        self.fc_layer = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64 * 7 * 7, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(128, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layer(x)\n",
        "        x = self.fc_layer(x)\n",
        "        return x\n",
        "\n",
        "model = CNN().to(device)\n",
        "print(model)\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 4. Loss Function and Optimizer\n",
        "# ----------------------------------------------------\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 5. Training Loop\n",
        "# ----------------------------------------------------\n",
        "epochs = 5\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass & update\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 6. Evaluation (Accuracy on Test Set)\n",
        "# ----------------------------------------------------\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f\"Test Accuracy: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "id": "PhJNfrNQbrIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.9. Given a custom image dataset stored in a local directory, write code using\n",
        "Keras ImageDataGenerator to preprocess and train a CNN model.\n",
        "\n",
        "Answer ->>"
      ],
      "metadata": {
        "id": "bYE1TAWsaUcX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n"
      ],
      "metadata": {
        "id": "lTeU8Ew_aooG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory paths\n",
        "train_dir = \"dataset/train/\"\n",
        "val_dir   = \"dataset/val/\"\n",
        "\n",
        "# Image properties\n",
        "img_height = 150\n",
        "img_width = 150\n",
        "batch_size = 32\n",
        "\n",
        "# ---------------------------------------\n",
        "# Data Augmentation for Training\n",
        "# ---------------------------------------\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,               # normalize pixel values\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    shear_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "# ---------------------------------------\n",
        "# Validation Data (NO augmentation)\n",
        "# ---------------------------------------\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n"
      ],
      "metadata": {
        "id": "BpPDyh_na0JI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\"\n",
        ")\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "qyBFBaHTa3G3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "\n",
        "    Conv2D(32, (3,3), activation='relu', padding='same',\n",
        "           input_shape=(img_height, img_width, 3)),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(),\n",
        "\n",
        "    Conv2D(64, (3,3), activation='relu', padding='same'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(),\n",
        "\n",
        "    Conv2D(128, (3,3), activation='relu', padding='same'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(train_generator.num_classes, activation='softmax')\n",
        "])\n"
      ],
      "metadata": {
        "id": "GTtcCkYra6gU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "4n5gdJA3a9ZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=20,\n",
        "    validation_data=val_generator\n",
        ")\n"
      ],
      "metadata": {
        "id": "tb8OzqqTbCHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss, acc = model.evaluate(val_generator)\n",
        "print(\"Validation Accuracy:\", acc)\n",
        "print(\"Validation Loss:\", loss)\n"
      ],
      "metadata": {
        "id": "bpCqz0V9bDHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.10. You are working on a web application for a medical imaging startup. Your\n",
        "task is to build and deploy a CNN model that classifies chest X-ray images into “Normal”\n",
        "and “Pneumonia” categories. Describe your end-to-end approach–from data preparation\n",
        "and model training to deploying the model as a web app using Streamlit.\n",
        "\n",
        "\n",
        "Answer ->>\n",
        "\n",
        "Below is a complete end-to-end approach to building, training, and deploying a CNN-based Chest X-ray classifier (Normal vs. Pneumonia) as a Streamlit web app.\n",
        "\n",
        "1. Data preperation      \n",
        "    - Dataset\n",
        "    - Processing steps\n",
        "\n",
        "2. CNN model training\n",
        "    - using transfer learning\n",
        "    - compile\n",
        "    - Train\n",
        "    - Fine-tuning\n",
        "3. Model Evaluation\n",
        "\n",
        "4. Save the tained model\n",
        "\n",
        "5. Building the Streamlit Web Application\n",
        "    - Folder structure\n",
        "\n",
        "6. Streamlit app code\n",
        "    - Load model\n",
        "    - File uploader UI\n",
        "    - Prediction function\n",
        "    - Main app logic\n",
        "\n",
        "7. Deployment Options\n",
        "    - Deploy using streamlit cloud\n",
        "    - Deploy on AWS EC2\n",
        "    - Containerized deployment\n",
        "    \n",
        "8. Security , Privacy & Compliance\n"
      ],
      "metadata": {
        "id": "4v0cfy5bbHZ8"
      }
    }
  ]
}